LLM_PROVIDER=ollama
OLLAMA_API_BASE=http://localhost:11434

# Model Configuration
MODEL=mistral

# Advanced: Multi-Model Support (optional)
# Uncomment to use different models for different tasks:
# PRIMARY_MODEL=qwen2.5:7b
# SECONDARY_MODEL=mistral
